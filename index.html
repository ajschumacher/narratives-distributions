<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html> <head>
 <link rel="stylesheet" href="typical.css" type="text/css" >
     <title>A Table of Narratives and Generated Distributions</title>


<!-- LaTeX math -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true}});
</script>

     </head><body>
     <h1>A Table of Narratives and Generated Distributions</h1>


<P>
This project lists open-form narratives and the closed-form distributions that
approximate them. Its intent is to help you build estimable statistical models on a sound
micro-level foundation.</p>

<p>Here is a simple example of going from a real-world situation to an estimable mathematical model:</p>

<p><div class="h2d"><a name="samp"></a><h6>A sample narrative</h6><p>
        
Narrative: Make a large series of independent, identically distributed (iid) draws from a source.
Take the mean of those draws.</p>

<p>Distribution: The distribution of repeated means will be a Normal distribution.</p>

<p></div></p>

<p>If you wanted to write a simulation, in which individual agents each experience some iid
shock and their mean level is measured and reported, this wonderful piece of mathematics
just saved you the trouble.</p>

<p>Now you can focus your energeies on the more novel parts of the storyline. Of course,
those too may have closed-form shortcuts that save you the trouble of writing down an
open-form simulation. Your final model may wind up being a combination of closed-form
submodels.</p>

<p>This project is an index of narratives that have closed-form equivalents, intended to help
users develop their </p>

<p>Little, if any, of this is novel, and every narrative-to-distribution should have a
reference to an existing work, (including Wikipedia, because this is uncontroversial,
textbook stuff). However, it is being presented in what seems to be a novel way, to
facilitate the development of detailed micro-level narratives using known bulding
blocks where they are available.</p>

<p>The fact that we are relying on so many existing sources means that we don't need to
provide proofs here, unless they are useful for elucidating the transformation.</p>

<p>Also, estimation is often not a trivial matter. Some of these examples may break for
small $N$. We may add these notes later, but at this stage it would be nice to just get
down as many narratives as possible, and leave the estimation details to the references.</p>

<p>$\def\Re{{\mathbb R}}
        \def\datas{{\mathbb D}}
        \def\params{{\mathbb P}}
        \def\models{{\mathbb M}}
        \def\mod#1{M_{#1}}$</p>

<h6>Notation</h6><p> The Normal distribution is a mapping of the form $f:(\mu, \sigma, x)
\to \Re^+$. We could also fix $\mu$ and $\sigma$, which produces a univariate function
$f:x\to\Re^+$, which makes use of a meta-function of the form $N:(\mu, \sigma) \to
(f:x\to\Re^+)$. Textbooks (and Wikipedia) often characterize â€œthe Gaussian distribution''
to indicate this meta-function that produces a series of Gaussian distribution functions
depending on the values to which $\mu$ and $\sigma$ have been fixed.</p>

<p>This document defines the Gaussian distribution as the mapping $f:(\mu, \sigma, x) \to \Re^+$.
Notationally, this means that the distribution would be written with three inputs,
$f(\mu, \sigma, x)$, where we understand all but the last inputs to be parameters, and the
last input to be observed data. Grammaticaly, this means that a distribution is a unique
entity (a single point in the space of functions), not a collection or class of
similar entities, and this implies that distribution names should be capitalized under
common English grammatical rules.</p>

<p>It would be equally coherent to describe the family of Gaussian distributions, where
fixing $\mu=3$ and $\sigma=2$ (for example) produces a single Gaussian distribution,
often notated with something like ${\cal N}(3, 2)$, with the understanding that this
is a univariate function $x\to\Re^+$.  In the notation here, that distribution would
be written as ${\cal N}(3, 2, x)$.</p>

<p></P>

<P>
<h1>Aggregation</h1><p></p>

<p><h2>Coin flips and die rolls</h2><p></p>

<p><div class="h2d"><a name="bernie"></a><h6>Draws from a binary option </h6><p>
        
Narrative: The coin-flip: one event occurs with probability $p$.</p>

<p>Distribution: This defines the Bernoulli Distribution, which is one with probability $p$ and zero with
probability $1-p$. The variance of a Bernoulli Distribution is $p(1-p)$.</p>

<p></div></p>

<p><div class="h2d"><a name="binom"></a><h6>Multiple draws from a binary option </h6><p>
        
Narrative: Draw $N$ events with probability $p$.</p>

<p>Distribution: The Binomial$(N, p)$ Distribution.
The mean is $Np$ and the variance is $Np(1-p)$.</p>

<p></div></p>

<p><div class="h2d"><a name="bernie"></a><h6>Draws from a more-than-binary option </h6><p>
        
Narrative: The die-roll: each observation is from a list of possible outcomes, each with its own
probability of occurring, ${\bf p} = [p_1, p_2, ..., p_k]$. Exactly one event happens each time, so 
$\sum_{i=1}^k p_i = 1$. We make $n$ draws. What is the $k$-dimensional vector of observed
outcomes?</p>

<p>Distribution: Multinomial$(n, {\bf p})$.</p>

<p></div></p>

<p><div class="h2d"><a name="negbinom"></a><h6>Draws without replacement </h6><p>
        
Narrative: Start with a pool of $s$ successes and $f$ failures, so $N=s+f$, and the Bernoulli $p=s/N$.
What are the odds that we get $x$ successes from $n$ draws without replacement?</p>

<p>Distribution: Negative binomial$(s, f, n, x)$</p>

<p></div></p>

<p><h2>Central Limit Theorems</h2><p></p>

<p><div class="h2d"><a name="clt"></a><h6>Mean of univariate iid observations </h6><p>
        
Narrative: Make a large series of independent, identically distributed (iid) draws from a source.
Report the mean of those draws.</p>

<p>Distribution: As $N\to\infty$, the distribution of repeated means will be a Normal Distribution, with mean $\mu=\sum x/N$ and $\sigma = \sum (x-\mu)^2/N$. 
(<a href="#klemens:modeling">klemens:modeling</a>)</p>

<p></div></p>

<p><div class="h2d"><a name="clt"></a><h6>Product of univariate iid observations </h6><p>
        
Narrative: Begin with a value $x$.
Make a large series of independent, identically distributed (iid) draws from a source.
Report the product, $x\cdot d_1 \cdot d_2 ...$.</p>

<p>Distribution: As $N\to\infty$, the distribution of products will be a Lognormal Distribution, with mean $\mu=\ln(x) +(\sum_i \ln(d_i))/N$ and $\sigma = \sum_i (\ln(d_i)-\mu)^2/N$. That is, the log of the products will be Normally distributed, and $\mu$ and $\sigma$ indicate the mean and standard deviation of the log.
(<a href="#klemens:modeling">klemens:modeling</a>)</p>

<p></div></p>

<p></P>

<P>
<h1>Wait times and frequency</h1><p></p>

<p><h2>Discrete</h2><p></p>

<p><div class="h2d"><a name="geometric"></a><h6>Wait until one success</h6><p>
        
Narrative: One <a href="#bernie">Bernoulli</a> (coin-flip) draw per period. What is the likelihood that the
first event will occur at period $x$? (<a href="#goswami:rao">goswami:rao, p 9</a>)</p>

<p>Distribution: <a href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric</a>$(p, x)$</p>

<p></div></p>

<p><div class="h2d"><a name="negbinom"></a><h6>Wait until $k$ successes</h6><p>
        
Narrative: One <a href="#bernie">Bernoulli</a> (coin-flip) draw per period. What is the likelihood that we will have to wait
$x$ draws before we observe $k$ successes?</p>

<p>Distribution: <a href="http://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative binomial</a>$(k, p, x)$</p>

<p></div></p>

<p><div class="h2d"><a name="markovgeometric"></a><h6>Markov chain stabilization</h6><p>
        
Narrative: A system is represented by a column vector of states ${\bf S}$, which
changes states according to a Markov matrix $P$. What is the likelihood that it
will take $t$ steps to reach a steady state, where ${\bf S}_{t+1} = {\bf S}_t\cdot P = {\bf S}_t$?</p>

<p>Distribution: The Markov Geometric Distribution,
$$MGD(R, Q, t)={\bf 1}'(I-tR)^{-1}(tQ){\bf 1},$$
where $Q$ is the diagonal matrix matching the diagonal of $P$, $R$ is the
off-diagonal of $P$ (so $R=P-Q$), ${\bf 1}$ is an appropriately-sized column vector of ones,
and $I$ and appropriately-sized identity matrix.
(<a href="#gani:jerwood">gani:jerwood, eqn 2.9</a>), notation via (<a href="#goswami:rao">goswami:rao, p 197</a>).</p>

<p></div></p>

<p><div class="h2d"><a name="poisson"></a><h6>Events per period</h6><p>
        
Narrative: Independent events (rainy day, landmine, bad data) occur at the
mean rate of $\lambda$ events per span (of time, space, et cetera). What is the
probability that there will be $t$ events in a single span?</p>

<p>Distribution: Poisson$(\lambda, t)$</p>

<p></div></p>

<p><h2>Continuous</h2><p></p>

<p><div class="h2d"><a name="eponential"></a><h6>Wait until a success</h6><p>
        
Narrative: Events occur via a <a href="#poisson">Poisson</a> process ($\lambda$ events per time/space span).
What is the likelihood that the first event will occur within $x$ periods?</p>

<p>Distribution: Exponential$(\lambda, x)$</p>

<p></div></p>

<p><div class="h2d"><a name="gamma"></a><h6>Wait until $k$ successes</h6><p>
        
Narrative: Events are a <a href="#poisson">Poisson</a> process: $\lambda$ events per time period. What is the
likleihood that we will observe $x$ events in a span of $k$ time units?</p>

<p>Spatial version: $\lambda$ events per spatial unit. What is the likelihood that we observe $x$ events over a distance or area of $k$ units?</p>

<p>Distribution: Gamma$(k, \lambda, x)$</p>

<p></div></p>

<p><div class="h2d"><a name="weibull"></a><h6>Wait until a success, but time scale is not even or success odds change</h6><p>
        
Narrative: Events occur via a <a href="#poisson">Poisson</a>-like process ($\lambda$ events per time/space span), 
so the likelihood of observing an event within $x$ periods is Exponential$(\lambda, x)$.
But the time scale is distorted, so $y=x^{1/\gamma}$. [Note that an additive or
multiplicative distortion would still give us an Exponential Distribution.]</p>

<p>Another way to describe this is that the event rate is  changing with time. If $\gamma > 1$,
events are more likely after some time. If $\gamma < 1$, events are more likely to occur early.       
(<a href="#casella:berger">casella:berger, p 103</a>)</p>

<p>Distribution: Weibull$(\gamma, \lambda, x)$.</p>

<p></div></p>

<p></P>

<P>
<h1>Order statistics</h1><p></p>

<p><div class="h2d"><a name="betafromuniform"></a><h6>The $N$th largest draw from a Uniform</h6><p>
        
Narrative: The first <em>order statistic</em> of a set of numbers $x$ is the smallest number in the
set; the second is the next-to-smallest, up to the largest order statistic, which
is $\max(x)$.</p>

<p>The $\alpha+\beta-1$ elements of $x$ are drawn from a Uniform$[0, 1]$ Distribution.</p>

<p>Distribution: The $\alpha$th order statistic has a Beta$(\alpha,\beta)$ Distribution.</p>

<p></div></p>

<p></P>

<P>
<h1>Other</h1><p></p>

<p><div class="h2d"><a name="uniform"></a><h6>Everything is equally likely</h6><p>
        
Narrative: We know the upper and lower bounds are $u$ and $l$, but believe that any draw within that
range is as likely as any other draw within that range.</p>

<p>Distribution: Uniform$(l, u)$</p>

<p></div></p>

<p></P>
<h2>Bibliography</h2>
</p><p> [<a href="#casella:berger">casella:berger</a>] Casella, G. and R. L. Berger (1990). <em>Statistical Inference</em>. Duxbury Press.</p><p> [<a href="#gani:jerwood">gani:jerwood</a>] Gani, J. and D. Jerwood (1971). Markov chain methods in chain binomial epidemic models. <em>Biometrics</em> <em>27</em>(3), 591--603.</p><p> [<a href="#goswami:rao">goswami:rao</a>] Goswami, A. and B. Rao (2006). <em>A Course in Applied Stochastic Processes</em>. Number 40 in Texts and Readings in Mathematics. Hindustan Book  Agency.</p><p> [<a href="#klemens:modeling">klemens:modeling</a>] Klemens, B. (2008). <em>Modeling with Data: Tools and Techniques for Statistical  Computing</em>. Princeton University Press.</body></html>
