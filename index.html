<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html> <head>
 <link rel="stylesheet" href="typical.css" type="text/css" >
     <title>A Table of Narratives and Generated Distributions</title>


<!-- LaTeX math -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true}});
</script>

     </head><body>
     <h1>A Table of Narratives and Generated Distributions</h1>


<P>
This project lists open-form narratives and the closed-form distributions that
approximate them. Its intent is to help you build estimable statistical models on a sound
micro-level foundation.</p>

<p>Here is a simple example of going from a real-world situation to an estimable mathematical model:</p>

<p><div class="h2d"><a name="samp"></a><h6>A sample narrative</h6><p>
        
Narrative: Make a large series of independent, identically distributed (iid) draws from a source.
Take the mean of those draws.</p>

<p>Distribution: The distribution of repeated means will be a Normal distribution.</p>

<p></div></p>

<p>If you wanted to write a simulation, in which individual agents each experience some iid
shock and their mean level is measured and reported, this wonderful piece of mathematics
just saved you the trouble.</p>

<p>Now you can focus your energeies on the more novel parts of the storyline. Of course,
those too may have closed-form shortcuts that save you the trouble of writing down an
open-form simulation. Your final model may wind up being a combination of closed-form
submodels.</p>

<p>This project is an index of narratives that have closed-form equivalents, intended to help
users develop their </p>

<p>Little, if any, of this is novel, and every narrative-to-distribution should have a
reference to an existing work, (including Wikipedia, because this is uncontroversial,
textbook stuff). However, it is being presented in what seems to be a novel way, to
facilitate the development of detailed micro-level narratives using known bulding
blocks where they are available.</p>

<p>The fact that we are relying on so many existing sources means that we don't need to
provide proofs here, unless they are useful for elucidating the transformation.</p>

<p>Also, estimation is often not a trivial matter. Some of these examples may break for
small $N$. We may add these notes later, but at this stage it would be nice to just get
down as many narratives as possible, and leave the estimation details to the references.</p>

<p></P>

<P>
<h1>Aggregation</h1><p></p>

<p><h2>Central Limit Theorems</h2><p></p>

<p><div class="h2d"><a name="bernie"></a><h6>Draws from a binary option </h6><p>
        
Narrative: The coin-flip: one event occurs with probability $p$.</p>

<p>Distribution: This defines the Bernoulli distribution, which is one with probability $p$ and zero with
probability $1-p$. The variance of a Bernoulli distribution is $p(1-p)$.</p>

<p></div></p>

<p><div class="h2d"><a name="binom"></a><h6>Multiple draws from a binary option </h6><p>
        
Narrative: Draw $N$ events with probability $p$.</p>

<p>Distribution: The Binomial$(N, p)$ distribution.
The mean is $Np$ and the variance is $Np(1-p)$.</p>

<p></div></p>

<p><div class="h2d"><a name="bernie"></a><h6>Draws from a more-than-binary option </h6><p>
        
Narrative: The die-roll: each observation is from a list of possible outcomes, each with its own
probability of occurring, ${\bf p} = [p_1, p_2, ..., p_k]$. Exactly one event happens each time, so 
$\sum_{i=1}^k p_i = 1$. We make $n$ draws. What is the $k$-dimensional vector of observed
outcomes?</p>

<p>Distribution: Multinomial$(n, {\bf p})$ distribution.</p>

<p></div></p>

<p><div class="h2d"><a name="negbinom"></a><h6>Draws without replacement </h6><p>
        
Narrative: Start with a pool of $s$ successes and $f$ failures, so $N=s+f$, and the Bernoulli $p=s/N$.
What are the odds that we get $x$ successes from $n$ draws without replacement?</p>

<p>Distribution: Negative binomial$(s, f, n)(x)$</p>

<p></div></p>

<p><div class="h2d"><a name="clt"></a><h6>Mean of univariate iid observations </h6><p>
        
Narrative: Make a large series of independent, identically distributed (iid) draws from a source.
Report the mean of those draws.</p>

<p>Distribution: As $N\to\infty$, the distribution of repeated means will be a Normal distribution, with mean $\mu=\sum x/N$ and $\sigma = \sum (x-\mu)^2/N$. 
(<a href="#klemens:modeling">klemens:modeling</a>)</p>

<p></div></p>

<p></P>

<P>
<h1>Wait times and frequency</h1><p></p>

<p><div class="h2d"><a name="negbinom"></a><h6>Wait until a success--discrete</h6><p>
        
Narrative: One <a href="#bernie">Bernoulli</a> (coin-flip) draw per period. What is the likelihood that we will have to wait
$x$ draws before we observe $k$ successes?</p>

<p>Distribution: <a href="http://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative binomial</a>$(k, p)(x)$</p>

<p></div></p>

<p><div class="h2d"><a name="eponential"></a><h6>Wait until a success--continuous</h6><p>
        
Narrative: Events occur via a <a href="#poisson">Poisson</a> process ($\lambda$ events per time/space span).
What is the likelihood that the first event will occur within $x$ periods?</p>

<p>Distribution: Exponential$(\lambda)(x)$</p>

<p></div></p>

<p><div class="h2d"><a name="gamma"></a><h6>Wait until $k$ successes--continuous</h6><p>
        
Narrative: Events are a <a href="#poisson">Poisson</a> process: $\lambda$ events per time period. What is the
likleihood that we will observe $x$ events in a span of $k$ time units?</p>

<p>Spatial version: $\lambda$ events per spatial unit. What is the likelihood that we observe $x$ events over a distance or area of $k$ units?</p>

<p>Distribution: Gamma$(k, \lambda)(x)$</p>

<p></div></p>

<p><div class="h2d"><a name="poisson"></a><h6>Events per period</h6><p>
        
Narrative: Independent events (rainy day, landmine, bad data) occur at the
mean rate of $\lambda$ events per span (of time, space, et cetera). What is the
probability that there will be $t$ events in a single span?</p>

<p>Distribution: Poisson$(\lambda)(x)$</p>

<p></div></p>

<p></P>

<P>
<h1>Order statistics</h1><p></p>

<p><div class="h2d"><a name="betafromuniform"></a><h6>The $N$th largest draw from a Uniform</h6><p>
        
Narrative: The first <em>order statistic</em> of a set of numbers $x$ is the smallest number in the
set; the second is the next-to-smallest, up to the largest order statistic, which
is $\max(x)$.</p>

<p>The $\alpha+\beta-1$ elements of $x$ are drawn from a Uniform$[0, 1]$ distribution.</p>

<p>Distribution: The $\alpha$th order statistic has a Beta$(\alpha,\beta)$ distribution.</p>

<p></div></p>

<p></P>

<P>
<h1>Other</h1><p></p>

<p><div class="h2d"><a name="uniform"></a><h6>Everything is equally likely</h6><p>
        
Narrative: We know the upper and lower bounds are $u$ and $l$, but believe that any draw within that
range is as likely as any other draw within that range.</p>

<p>Distribution: Uniform$(l, u)$</p>

<p></div></p>

<p></P>
<h2>Bibliography</h2>
</p><p> [<a href="#elbers:lanjouw">elbers:lanjouw</a>] Elbers, C., J. O. Lanjouw, and P. Lanjouw (2003). Micro-level estimation of poverty and inequality. <em>Econometrica</em> <em>71</em>(1), pp. 355--364.</p><p> [<a href="#fay:herriot">fay:herriot</a>] Fay, R. E. and R. A. Herriot (1979). Estimates of income for small places: An application of  James-Stein procedures to census data. <em>Journal of the American Statistical Association</em> <em> 74</em>(366), pp. 269--277.</p><p> [<a href="#klemens:modeling">klemens:modeling</a>] Klemens, B. (2008). <em>Modeling with Data: Tools and Techniques for Statistical  Computing</em>. Princeton University Press.</p><p> [<a href="#molina:rao">molina:rao</a>] Molina, I. and J. N. K. Rao (2010). Small area estimation of poverty indicators. <em>Canadian Journal of Statistics</em> <em>38</em>(3), 369--385.</body></html>
